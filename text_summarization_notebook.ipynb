{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization for Long Documents using Hugging Face\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Traditional summarization models like BART or T5 have token limitations that prevent them from processing long documents directly. This notebook implements a sliding window approach to break long texts into manageable chunks, summarize each chunk, and combine the results into a coherent final summary.\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained BART Model\n",
    "\n",
    "We'll use Facebook's BART model fine-tuned on CNN/Daily Mail dataset for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Sliding Window Summarization\n",
    "\n",
    "This function handles long documents by splitting them into overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_summarize(text, max_input_length=1024, max_output_length=200, stride=512):\n",
    "    \"\"\"\n",
    "    Summarizes a long document using a sliding window approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The input long text that needs to be summarized.\n",
    "    - max_input_length: The max token length for the model's input (default: 1024).\n",
    "    - max_output_length: The max token length for the model's output (default: 200).\n",
    "    - stride: The sliding window stride to overlap chunks (default: 512).\n",
    "    \n",
    "    Returns:\n",
    "    - final_summary: The combined summary from all chunks.\n",
    "    \"\"\"\n",
    "    # Tokenize to get accurate length\n",
    "    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=True)\n",
    "    \n",
    "    # If text is short enough, summarize directly\n",
    "    if len(tokens) <= max_input_length:\n",
    "        summary = summarizer(text, max_length=max_output_length, min_length=50, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    \n",
    "    # Split the text into overlapping chunks\n",
    "    chunks = []\n",
    "    chunk_texts = []\n",
    "    \n",
    "    for i in range(0, len(tokens), max_input_length - stride):\n",
    "        chunk_tokens = tokens[i:i + max_input_length]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunk_texts.append(chunk_text)\n",
    "    \n",
    "    print(f\"Document split into {len(chunk_texts)} chunks\")\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunk_texts):\n",
    "        print(f\"Summarizing chunk {i+1}/{len(chunk_texts)}...\")\n",
    "        summary = summarizer(chunk, max_length=max_output_length, min_length=50, do_sample=False)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "    \n",
    "    # Join summaries from all chunks\n",
    "    intermediate_summary = ' '.join(summaries)\n",
    "    \n",
    "    # If the combined summary is still too long, summarize it again\n",
    "    intermediate_tokens = tokenizer.encode(intermediate_summary, truncation=False, add_special_tokens=True)\n",
    "    if len(intermediate_tokens) > max_input_length:\n",
    "        print(\"Combined summary is long, performing final summarization...\")\n",
    "        final_summary = summarizer(intermediate_summary, max_length=max_output_length*2, min_length=100, do_sample=False)\n",
    "        return final_summary[0]['summary_text']\n",
    "    \n",
    "    return intermediate_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(original_text, summary):\n",
    "    \"\"\"Display the original text and summary in a formatted way.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ORIGINAL TEXT LENGTH:\", len(original_text), \"characters\")\n",
    "    print(\"=\" * 80)\n",
    "    print(textwrap.fill(original_text[:500] + \"...\", width=80))\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY LENGTH:\", len(summary), \"characters\")\n",
    "    print(\"=\" * 80)\n",
    "    print(textwrap.fill(summary, width=80))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Long Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample long text - Replace with your actual document\n",
    "long_text = \"\"\"\n",
    "The field of artificial intelligence has undergone remarkable transformations since its inception in the 1950s. \n",
    "What began as theoretical discussions among mathematicians and computer scientists has evolved into one of the \n",
    "most influential technologies of the 21st century. The journey from simple rule-based systems to today's \n",
    "sophisticated deep learning models represents decades of innovation, setbacks, and breakthroughs.\n",
    "\n",
    "In the early days, AI researchers were optimistic about creating machines that could think like humans within \n",
    "a few decades. The Dartmouth Conference of 1956, often considered the birth of AI as a field, brought together \n",
    "pioneers like John McCarthy, Marvin Minsky, and Claude Shannon. They believed that \"every aspect of learning \n",
    "or any other feature of intelligence can in principle be so precisely described that a machine can be made to \n",
    "simulate it.\"\n",
    "\n",
    "The 1960s and 1970s saw the development of expert systems and symbolic AI. These systems used predefined rules \n",
    "and logic to solve specific problems. ELIZA, created by Joseph Weizenbaum in 1964, was one of the first \n",
    "chatbots that could engage in seemingly intelligent conversation. However, it quickly became apparent that \n",
    "these rule-based approaches had significant limitations.\n",
    "\n",
    "The first \"AI winter\" occurred in the 1970s when funding dried up due to unmet expectations. Researchers had \n",
    "promised too much too soon, and the technology couldn't deliver on those promises. This period taught the AI \n",
    "community valuable lessons about managing expectations and the complexity of human intelligence.\n",
    "\n",
    "The 1980s brought a resurgence with the popularity of expert systems in business applications. Companies \n",
    "invested heavily in AI systems for tasks like financial analysis and medical diagnosis. Japan's Fifth \n",
    "Generation Computer Project aimed to create intelligent computers by the 1990s. However, these systems were \n",
    "brittle and couldn't handle situations outside their programmed expertise.\n",
    "\n",
    "The second AI winter in the late 1980s and early 1990s again saw reduced funding and interest. However, this \n",
    "period also saw important theoretical developments. Researchers began exploring neural networks more seriously, \n",
    "inspired by the structure of the human brain. Geoffrey Hinton, Yann LeCun, and others laid the groundwork for \n",
    "what would become deep learning.\n",
    "\n",
    "The late 1990s and 2000s marked a turning point. The internet provided vast amounts of data, computational \n",
    "power increased exponentially following Moore's Law, and new algorithms made training neural networks more \n",
    "practical. IBM's Deep Blue defeating world chess champion Garry Kasparov in 1997 demonstrated AI's potential \n",
    "in specialized domains.\n",
    "\n",
    "The real breakthrough came in the 2010s with deep learning. In 2012, AlexNet dramatically improved image \n",
    "recognition performance in the ImageNet competition. This success sparked renewed interest and investment in AI. \n",
    "Companies like Google, Facebook, and Microsoft established AI research labs and began integrating AI into their \n",
    "products.\n",
    "\n",
    "The transformer architecture, introduced in 2017, revolutionized natural language processing. Models like BERT \n",
    "and GPT demonstrated unprecedented language understanding capabilities. These models could generate coherent \n",
    "text, answer questions, and even write code. The scale of these models grew rapidly, from millions to billions \n",
    "and now trillions of parameters.\n",
    "\n",
    "Today, AI is ubiquitous. It powers recommendation systems, virtual assistants, autonomous vehicles, and \n",
    "medical diagnostic tools. The COVID-19 pandemic accelerated AI adoption in healthcare, with AI systems helping \n",
    "to develop vaccines and predict disease spread. However, this widespread deployment has also raised important \n",
    "ethical questions about bias, privacy, and the societal impact of automation.\n",
    "\n",
    "Looking forward, the field faces both exciting opportunities and significant challenges. Researchers are \n",
    "working on making AI more explainable, robust, and aligned with human values. The goal of artificial general \n",
    "intelligence (AGI) remains elusive but continues to drive research. As we stand at this inflection point, it's \n",
    "clear that AI will play an increasingly central role in shaping our future, making it crucial that we develop \n",
    "and deploy these technologies responsibly.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "print(\"Starting summarization process...\")\n",
    "final_summary = sliding_window_summarize(long_text, max_input_length=1024, max_output_length=150, stride=512)\n",
    "\n",
    "# Display results\n",
    "display_summary(long_text, final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage: Customizing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with different parameters for more detailed summaries\n",
    "detailed_summary = sliding_window_summarize(\n",
    "    long_text, \n",
    "    max_input_length=1024,    # Maximum tokens per chunk\n",
    "    max_output_length=250,    # Longer summaries per chunk\n",
    "    stride=256               # More overlap between chunks\n",
    ")\n",
    "\n",
    "print(\"DETAILED SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(textwrap.fill(detailed_summary, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_summarize_documents(documents, **kwargs):\n",
    "    \"\"\"Summarize multiple documents and return results.\"\"\"\n",
    "    summaries = {}\n",
    "    \n",
    "    for i, (doc_name, doc_text) in enumerate(documents.items()):\n",
    "        print(f\"\\nProcessing document {i+1}/{len(documents)}: {doc_name}\")\n",
    "        summaries[doc_name] = sliding_window_summarize(doc_text, **kwargs)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Example usage\n",
    "documents = {\n",
    "    \"Document 1\": long_text,\n",
    "    # Add more documents here\n",
    "}\n",
    "\n",
    "all_summaries = batch_summarize_documents(documents, max_output_length=200)\n",
    "\n",
    "# Display all summaries\n",
    "for doc_name, summary in all_summaries.items():\n",
    "    print(f\"\\n{doc_name} Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(textwrap.fill(summary, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save summaries to file\n",
    "def save_summaries(summaries, filename=None):\n",
    "    \"\"\"Save summaries to a JSON file.\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"summaries_{timestamp}.json\"\n",
    "    \n",
    "    output = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model\": model_name,\n",
    "        \"summaries\": summaries\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Summaries saved to {filename}\")\n",
    "\n",
    "# Example\n",
    "save_summaries({\"example_document\": final_summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU acceleration (if available)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Using GPU for acceleration\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Batch processing for multiple chunks (more efficient)\n",
    "def optimized_sliding_window_summarize(text, batch_size=2, **kwargs):\n",
    "    \"\"\"Process multiple chunks in batches for better performance.\"\"\"\n",
    "    # Implementation would process multiple chunks simultaneously\n",
    "    # This is a placeholder for the concept\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Handle long documents that exceed model token limits\n",
    "2. Use sliding window approach with overlapping chunks\n",
    "3. Combine chunk summaries into coherent final summaries\n",
    "4. Customize parameters for different use cases\n",
    "\n",
    "### Potential Improvements:\n",
    "- **Fine-tuning**: Train the model on domain-specific data for better summaries\n",
    "- **Preprocessing**: Clean text (remove URLs, formatting, etc.) before summarization\n",
    "- **Post-processing**: Apply additional NLP techniques to improve coherence\n",
    "- **Alternative Models**: Try T5, Pegasus, or other summarization models\n",
    "- **Evaluation**: Implement ROUGE scores to measure summary quality\n",
    "\n",
    "### Resources:\n",
    "- [Hugging Face Documentation](https://huggingface.co/docs/transformers)\n",
    "- [BART Paper](https://arxiv.org/abs/1910.13461)\n",
    "- [Summarization Task Guide](https://huggingface.co/tasks/summarization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}